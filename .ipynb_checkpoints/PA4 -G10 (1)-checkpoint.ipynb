{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6111534",
   "metadata": {},
   "source": [
    "# Linear Classifiers implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "60116f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.base import BaseEstimator\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aml_perceptron import Perceptron, SparsePerceptron\n",
    "import scipy.linalg.blas as blas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6aaf4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef1e2f",
   "metadata": {},
   "source": [
    "# Implementing the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "33ed2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=7000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "    \n",
    "        # convert the data to a list of tuples of (features, label)\n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        # create a random subset of indices of the data to be looped on,\n",
    "        # and avoid looping over the while dataset\n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        # initialize empty lists to add in the hingeloss and \n",
    "        # the score while looping over the rows of the data subset\n",
    "        hingeLoss=[]\n",
    "        scoreList=[]\n",
    "        \n",
    "        for i in range (self.n_iter):\n",
    "            \n",
    "            # increment the value t by 1\n",
    "            t = i+1\n",
    "            \n",
    "            # assign the features of the row to x and the corresponding label to y\n",
    "            x= data[T[i]][0]\n",
    "            y = data[T[i]][1]\n",
    "            \n",
    "            # calculate the value of the learning rate eta\n",
    "            eta = 1/(self.Lambda*t)\n",
    "            \n",
    "            # calculate the score\n",
    "            score = x.dot(self.w)\n",
    "            \n",
    "            # case when y*score is less than oner\n",
    "            if y*(score) <1:\n",
    "                self.w = (1-eta*self.Lambda)*self.w + eta*y*x\n",
    "                hingeLoss.append(1-y*score)\n",
    "                \n",
    "            else:\n",
    "                self.w = (1-eta*self.Lambda)*self.w\n",
    "                hingeLoss.append(0)\n",
    "            \n",
    "            #print the objective function value every 500 epochs\n",
    "            if i%500==0 and i != 0:\n",
    "                # print objective function at epoch i as \n",
    "                # average of hinge loss at \n",
    "                #i + the regularization paramete lambda/2*||w||^2\n",
    "                print( \"The objective function at epoch \" +\n",
    "                      str(i)+': '+str(np.mean(hingeLoss)+(self.Lambda/2)*(self.w.dot(self.w))))\n",
    "                    \n",
    "                    \n",
    "            scoreList.append(y*score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9436d000",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 2.145577527971463\n",
      "The objective function at epoch 1000: 1.405765143503714\n",
      "The objective function at epoch 1500: 1.1390472968221403\n",
      "The objective function at epoch 2000: 1.0164382834963646\n",
      "The objective function at epoch 2500: 0.9363627026417346\n",
      "The objective function at epoch 3000: 0.8803642195434528\n",
      "The objective function at epoch 3500: 0.83951486677521\n",
      "The objective function at epoch 4000: 0.8024794650250484\n",
      "The objective function at epoch 4500: 0.7753272343687765\n",
      "The objective function at epoch 5000: 0.7484251949423734\n",
      "The objective function at epoch 5500: 0.7319530405396902\n",
      "The objective function at epoch 6000: 0.7151860846674198\n",
      "The objective function at epoch 6500: 0.7013445809971559\n",
      "\n",
      "\n",
      "Training time of SVC algorithm: 0.87 sec.\n",
      "Accuracy of SVC algorithm: 0.8162.\n"
     ]
    }
   ],
   "source": [
    "# This function reads the corpus, \n",
    "#returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "    \n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,random_state=0)\n",
    "\n",
    "# function to calculate the training time and accuracy of the model    \n",
    "def model_report(pipeline, s):\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('\\n')\n",
    "    print('Training time of '+s+' algorithm: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy of '+s+' algorithm: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n",
    "\n",
    "\n",
    "# Set up the preprocessing steps for the SVC model\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVC()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89169e38",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8a183e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=7000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        loss = []\n",
    "        scoreList=[]\n",
    "        \n",
    "        for i in range (self.n_iter):\n",
    "            \n",
    "            t = i+1\n",
    "            \n",
    "            x= data[T[i]][0]\n",
    "            y = data[T[i]][1]\n",
    "            \n",
    "            eta = 1/(self.Lambda*t)\n",
    "            \n",
    "            score = x.dot(self.w)\n",
    "            \n",
    "            # compute gradient of the loss function\n",
    "            gradLoss = -y/(1+np.exp(y*score))*x\n",
    "            \n",
    "            # compute gradient of f(w,x,y)\n",
    "            gradientF = self.Lambda*self.w + gradLoss\n",
    "            \n",
    "            # update w = w- eta* gradient\n",
    "            self.w = self.w - eta*gradientF\n",
    "            \n",
    "            scoreList.append(y*score)\n",
    "      \n",
    "            loss.append(np.log(1+np.exp(-y*score)))\n",
    "            \n",
    "            if i%500==0 and i != 0:\n",
    "                # print objective function at epoch i as \n",
    "                # average of hinge loss at i + the regularization paramete lambda/2*||w||^2\n",
    "                print( \"The objective function at epoch \" +str(i)+': '+str(np.mean(loss)+(self.Lambda/2)*(self.w.dot(self.w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "061977b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 1.5944405836649074\n",
      "The objective function at epoch 1000: 1.0728520601312541\n",
      "The objective function at epoch 1500: 0.900906000658165\n",
      "The objective function at epoch 2000: 0.8127802350889239\n",
      "The objective function at epoch 2500: 0.7602792959342172\n",
      "The objective function at epoch 3000: 0.7227794841690319\n",
      "The objective function at epoch 3500: 0.6964600526289264\n",
      "The objective function at epoch 4000: 0.6771181698390193\n",
      "The objective function at epoch 4500: 0.6631386613609567\n",
      "The objective function at epoch 5000: 0.652575972489855\n",
      "The objective function at epoch 5500: 0.6432017392843276\n",
      "The objective function at epoch 6000: 0.6351934327027385\n",
      "The objective function at epoch 6500: 0.628909694349975\n",
      "\n",
      "\n",
      "Training time of Logistic Regression algorithm: 0.98 sec.\n",
      "Accuracy of Logistic Regression algorithm: 0.8044.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps for the logistic regression model.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6089a6",
   "metadata": {},
   "source": [
    "# BMaking your code more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2c38f",
   "metadata": {},
   "source": [
    "### (a) Faster linear algebra operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cd838",
   "metadata": {},
   "source": [
    "### Implementing the SVC algorithm with blas functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "faee18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlasSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=7000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        hingeLoss=[]\n",
    "        scoreList=[]\n",
    "        \n",
    "        for i in range (self.n_iter):\n",
    "            \n",
    "            t = i+1\n",
    "            \n",
    "            x= data[T[i]][0]\n",
    "            y = data[T[i]][1]\n",
    "            \n",
    "            eta = 1/(self.Lambda*t)\n",
    "            \n",
    "            # computing the score x.w\n",
    "            score = blas.ddot(x,self.w)\n",
    "            \n",
    "            #y.(x.w)\n",
    "            Yscore = blas.ddot(y,score)\n",
    "            \n",
    "            if Yscore <1:\n",
    "                self.w = blas.dscal((1-eta*self.Lambda),self.w) + eta*y*x\n",
    "                \n",
    "            else:\n",
    "                blas.dscal((1-eta*self.Lambda),self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a0253ff9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Blas SVC algorithm: 0.76 sec.\n",
      "Accuracy of Blas SVC algorithm: 0.8112.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    BlasSVC()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Blas SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513f224",
   "metadata": {},
   "source": [
    "Using blas functions for the SVC model implementation, the model achieved approximately the same accuracy as the first implementation of the model using numpy function in 87% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "93b35224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlasLogisticRegression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=7000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        loss = []\n",
    "        scoreList=[]\n",
    "        \n",
    "        for i in range (self.n_iter):\n",
    "            \n",
    "            t = i+1\n",
    "            \n",
    "            x= data[T[i]][0]\n",
    "            y = data[T[i]][1]\n",
    "            \n",
    "            eta = 1/(self.Lambda*t)\n",
    "            \n",
    "            # computing the score x.w\n",
    "            score = blas.ddot(x,self.w)\n",
    "            \n",
    "            # compute gradient of the loss function\n",
    "            gradLoss = -1/(1+np.exp(blas.ddot(y,score)))*blas.dscal(y,x)\n",
    "            \n",
    "            # compute gradient of f(w,x,y)\n",
    "            gradientF = self.Lambda*self.w + gradLoss\n",
    "            \n",
    "            # update w = w- eta* gradient\n",
    "            self.w = self.w - blas.dscal(eta,gradientF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "68d192f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Blas Logistic Regression algorithm: 0.79 sec.\n",
      "Accuracy of Blas Logistic Regression algorithm: 0.8070.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(), \n",
    "    BlasLogisticRegression()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Blas Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292d81d",
   "metadata": {},
   "source": [
    "Using blas functions for the Logistic Regression model implementation, the model achieved approximately the same accuracy as the first implementation of the model using numpy function in 80% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b4491",
   "metadata": {},
   "source": [
    "### (b) Using sparse vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26175b6c",
   "metadata": {},
   "source": [
    "#### The feature selector function SelectKbest removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "2c4735c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 1.769811772173894\n",
      "The objective function at epoch 1000: 1.3146129363860326\n",
      "The objective function at epoch 1500: 1.1347601767120277\n",
      "The objective function at epoch 2000: 1.041881640989938\n",
      "The objective function at epoch 2500: 0.9766763035746435\n",
      "The objective function at epoch 3000: 0.9359860521689838\n",
      "The objective function at epoch 3500: 0.9037627889963831\n",
      "The objective function at epoch 4000: 0.8833732912999087\n",
      "The objective function at epoch 4500: 0.8671499066425111\n",
      "The objective function at epoch 5000: 0.8588863832781202\n",
      "The objective function at epoch 5500: 0.8444535808692039\n",
      "The objective function at epoch 6000: 0.8321545749452501\n",
      "The objective function at epoch 6500: 0.8231578685731554\n",
      "\n",
      "\n",
      "Training time of SVC algorithm: 3.40 sec.\n",
      "Accuracy of SVC algorithm: 0.8053.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    Normalizer(), \n",
    "    SVC()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5a3a0d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 0.9840809335981773\n",
      "The objective function at epoch 1000: 0.7991783925883085\n",
      "The objective function at epoch 1500: 0.7411517347379536\n",
      "The objective function at epoch 2000: 0.7096782763605686\n",
      "The objective function at epoch 2500: 0.6920120507869819\n",
      "The objective function at epoch 3000: 0.6806792426753132\n",
      "The objective function at epoch 3500: 0.6726233658907448\n",
      "The objective function at epoch 4000: 0.6665198044137169\n",
      "The objective function at epoch 4500: 0.6621041208682088\n",
      "The objective function at epoch 5000: 0.6582117160003553\n",
      "The objective function at epoch 5500: 0.6548784424637133\n",
      "The objective function at epoch 6000: 0.6521658004650067\n",
      "The objective function at epoch 6500: 0.6498056255639919\n",
      "\n",
      "\n",
      "Training time of  Logistic Regression algorithm: 3.78 sec.\n",
      "Accuracy of  Logistic Regression algorithm: 0.8023.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    Normalizer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "model_report(pipeline,' Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8debb",
   "metadata": {},
   "source": [
    "By removing the feature selection function SelectKbest from the models training pipeline, we noticed that the models achieved approximately the same accuracy as when the function was included in the pipeline, which may mean that all the features (terms) are important for the model classification task performance, while the training time for each of both models (SVC and Logistic Regression) increase by approximately 4-5 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe7af7",
   "metadata": {},
   "source": [
    "### Training time and accuracy differences by adding both of the bigrams and unigrams to the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "820b43f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 1.8385828274076264\n",
      "The objective function at epoch 1000: 1.3607004697142502\n",
      "The objective function at epoch 1500: 1.1948642276613959\n",
      "The objective function at epoch 2000: 1.1160883024710175\n",
      "The objective function at epoch 2500: 1.0566366117416872\n",
      "The objective function at epoch 3000: 1.016735668177459\n",
      "The objective function at epoch 3500: 0.9898740732430968\n",
      "The objective function at epoch 4000: 0.9688693851768646\n",
      "The objective function at epoch 4500: 0.9538358649158737\n",
      "The objective function at epoch 5000: 0.9432699857197404\n",
      "The objective function at epoch 5500: 0.934047630492735\n",
      "The objective function at epoch 6000: 0.9278510660734998\n",
      "The objective function at epoch 6500: 0.9202408910638751\n",
      "\n",
      "\n",
      "Training time of SVC algorithm: 23.98 sec.\n",
      "Accuracy of SVC algorithm: 0.8044.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(), \n",
    "    SVC()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "51edb881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function at epoch 500: 0.9443979532810096\n",
      "The objective function at epoch 1000: 0.794764327456181\n",
      "The objective function at epoch 1500: 0.7457907069352735\n",
      "The objective function at epoch 2000: 0.7227993914046463\n",
      "The objective function at epoch 2500: 0.7089427651649233\n",
      "The objective function at epoch 3000: 0.6996861505410272\n",
      "The objective function at epoch 3500: 0.6928550034389482\n",
      "The objective function at epoch 4000: 0.6878418900081122\n",
      "The objective function at epoch 4500: 0.6841651601398835\n",
      "The objective function at epoch 5000: 0.6811251687894148\n",
      "The objective function at epoch 5500: 0.6787238416873461\n",
      "The objective function at epoch 6000: 0.6766620373295279\n",
      "The objective function at epoch 6500: 0.6750730332521439\n",
      "\n",
      "\n",
      "Training time of Logistic Regression algorithm: 28.29 sec.\n",
      "Accuracy of Logistic Regression algorithm: 0.8112.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cedf64",
   "metadata": {},
   "source": [
    "By specifying the unigrams and bigrams calculation in the TfIdf vectorization step in both of the models pipelines, the accuracies of each of the models stayed approximately the same, while the training time increased drastically for SVC from 3.4 secs to 24 secs and for Logistic Regression from 3.8 secs to 28 secs, and that's because now the model is trained using a much higher number of features as the pair combination between the terms of the text is now taken into consideration by the model in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8602b",
   "metadata": {},
   "source": [
    "### Using sparse functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1bc02",
   "metadata": {},
   "source": [
    "# Sparse SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "eae6019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2e8eef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        subData = [data[i] for i in T]\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in subData:\n",
    "                \n",
    "                t = i+1\n",
    "\n",
    "                eta = 1/(self.Lambda*t)\n",
    "\n",
    "                score = sparse_dense_dot(x,self.w)\n",
    "\n",
    "                Yscore = y*score\n",
    "\n",
    "                if Yscore <1:\n",
    "                    add_sparse_to_dense(x, self.w, y/self.Lambda)\n",
    "                    self.w = self.w - eta*self.Lambda*self.w\n",
    "                else:\n",
    "                    w = (1-eta*self.Lambda)*self.w\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "e8d44148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Sparse SVC algorithm: 9.39 sec.\n",
      "Accuracy of Sparse SVC algorithm: 0.7465.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    Normalizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    SparseSVC()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Sparse SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490797e",
   "metadata": {},
   "source": [
    "using sparse vectors for the mathematical operations included in the SVC model implementation, the model performed quite poorley where it finished the training phase in approximately 9 seconds and with a lower accuracy of approximately 4% than the numpy implementation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e7e8a",
   "metadata": {},
   "source": [
    "# Sparse LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "b2d5629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLogisticR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        subData = [data[i] for i in T]\n",
    "        \n",
    "        t=0\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in subData:\n",
    "                \n",
    "                t=t+1\n",
    "                \n",
    "                eta=1/(self.Lambda*t)\n",
    "                w=1-eta*self.Lambda*self.w             \n",
    "                add_sparse_to_dense(x , self.w,\n",
    "                                    y*eta/(1 + np.exp(y*(sparse_dense_dot(x, self.w)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "cff45951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Sparse Logistic Regression algorithm: 17.03 sec.\n",
      "Accuracy of Sparse Logistic Regression algorithm: 0.6949.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(), \n",
    "    SparseLogisticR()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Sparse Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66bb79",
   "metadata": {},
   "source": [
    "using sparse vectors for the mathematical operations included in the logistic regression model implementation, the model performed quite poorley where it finished the training phase in approximately 17 seconds and with a lower accuracy of approximately 10% than the numpy implementation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b80e8",
   "metadata": {},
   "source": [
    "### (c) Speeding up the scaling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e5c45cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLogisticRc(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        subData = [data[i] for i in T]\n",
    "        \n",
    "        t=0\n",
    "        \n",
    "        #initializing the scaling factor a \n",
    "        a = 1\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in subData:\n",
    "                \n",
    "                t=t+1\n",
    "                \n",
    "                eta=1/(self.Lambda*t)\n",
    "                \n",
    "                #replacing the vector scaling step\n",
    "                a = (1- eta*self.Lambda)*a\n",
    "                \n",
    "                #  division by zero exception, so we added an if statement to\n",
    "                # handle this case by removing the scaling factor if its equal to zero\n",
    "                if a == 0:\n",
    "                    w=1-eta*self.Lambda*self.w             \n",
    "                    add_sparse_to_dense(x , self.w, y*eta/(1 + np.exp(y*(sparse_dense_dot(x, self.w)))))  \n",
    "                    \n",
    "                else:\n",
    "                    add_sparse_to_dense(x , self.w, y*eta/(a*(1 + np.exp(y*(sparse_dense_dot(x, self.w))))))\n",
    "                    self.w *=a\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c067d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Sparse Logistic Regression algorithm: 17.19 sec.\n",
      "Accuracy of Sparse Logistic Regression algorithm: 0.6655.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(), \n",
    "    SparseLogisticRc()\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Sparse Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c6023",
   "metadata": {},
   "source": [
    "using the scaling factor <b>a</b> the model showed approximately no improvement than the model implementation using sparse vectors, so we investigate further the effect of replacing the other scaling operations with blas functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43a80f",
   "metadata": {},
   "source": [
    "### using blas functions for scaling and sparse vectors for other operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "005eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLogisticRc2(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,Lambda=0.001,eta=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.Lambda = Lambda\n",
    "        self.eta = eta\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        n_instances = X.shape[0]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        data = list(zip(X, Ye))\n",
    "        \n",
    "        T = np.random.choice(len(data),self.n_iter,replace = False)\n",
    "        \n",
    "        subData = [data[i] for i in T]\n",
    "        \n",
    "        t=0\n",
    "        \n",
    "        a = 1\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in subData:\n",
    "                \n",
    "                t=t+1\n",
    "                \n",
    "                eta=1/(self.Lambda*t)\n",
    "                \n",
    "                a = (1- eta*self.Lambda)*a\n",
    "                \n",
    "                #  division by zero exception, so we added an if statement to\n",
    "                # handle this case\n",
    "                if a == 0:\n",
    "                    blas.dscal(1-eta*self.Lambda,self.w)             \n",
    "                    add_sparse_to_dense(x , self.w, y*eta/(1 + np.exp(y*(sparse_dense_dot(x, self.w)))))  \n",
    "                    \n",
    "                else:\n",
    "                    add_sparse_to_dense(x , self.w, y*eta/(a*(1 + np.exp(y*(sparse_dense_dot(x, self.w))))))\n",
    "                    blas.dscal(a,self.w)\n",
    "                \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "6639f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training time of Sparse Logistic Regression with blas scaling functions algorithm: 15.77 sec.\n",
      "Accuracy of Sparse Logistic Regression with blas scaling functions algorithm: 0.7856.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(), \n",
    "    SparseLogisticRc2(n_iter=1000,Lambda=0.001,eta=0.001)\n",
    ")\n",
    "\n",
    "model_report(pipeline,'Sparse Logistic Regression with blas scaling functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c652e",
   "metadata": {},
   "source": [
    "by replacing the vector scaling operations with blas functions in the sparse Logistic Regression model, the model trainging time decreased slightly with 2 seconds, while the model's accuracy increased by approximately 12%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
